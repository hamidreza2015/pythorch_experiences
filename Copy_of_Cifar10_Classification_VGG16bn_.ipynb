{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Cifar10_Classification_VGG16bn_.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hamidreza2015/pythorch_experiences/blob/master/Copy_of_Cifar10_Classification_VGG16bn_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XoZynJvNOqox",
        "colab": {}
      },
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.models as models\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Yqy-nkngQg6j",
        "outputId": "0e41eb3b-4ef1-4800-937b-f39e7e495547",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Train, Validate, Test. Heavily inspired by Kevinzakka https://github.com/kevinzakka/DenseNet/blob/master/data_loader.py\n",
        "\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "\n",
        "\n",
        "\n",
        "# define transforms\n",
        "valid_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "])\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "])\n",
        "\n",
        "\n",
        "# load the dataset\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, \n",
        "            download=True, transform=train_transform)\n",
        "\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=True, \n",
        "            download=True, transform=valid_transform)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nK_2yRM0RJAG",
        "colab": {}
      },
      "source": [
        "batch_size = 64\n",
        "\n",
        "valid_size=0.1\n",
        "\n",
        "num_train = len(train_dataset)\n",
        "indices = list(range(num_train))\n",
        "split = int(np.floor(valid_size * num_train))\n",
        "\n",
        "\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
        "                batch_size=batch_size, sampler=train_sampler)\n",
        "\n",
        "valid_loader = torch.utils.data.DataLoader(train_dataset, \n",
        "                batch_size=batch_size, sampler=valid_sampler)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Asg_Qm0mrQOy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "\n",
        "__all__ = [\n",
        "    'VGG', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn',\n",
        "    'vgg19_bn', 'vgg19',\n",
        "]\n",
        "\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    '''\n",
        "    VGG model \n",
        "    '''\n",
        "    def __init__(self, features):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = features\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(512, 10),\n",
        "        )\n",
        "         # Initialize weights\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def make_layers(cfg, batch_norm=False):\n",
        "    layers = []\n",
        "    in_channels = 3\n",
        "    for v in cfg:\n",
        "        if v == 'M':\n",
        "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "        else:\n",
        "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
        "            if batch_norm:\n",
        "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
        "            else:\n",
        "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
        "            in_channels = v\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "cfg = {\n",
        "    'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', \n",
        "          512, 512, 512, 512, 'M'],\n",
        "}\n",
        "\n",
        "\n",
        "def vgg11():\n",
        "    \"\"\"VGG 11-layer model (configuration \"A\")\"\"\"\n",
        "    return VGG(make_layers(cfg['A']))\n",
        "\n",
        "\n",
        "def vgg11_bn():\n",
        "    \"\"\"VGG 11-layer model (configuration \"A\") with batch normalization\"\"\"\n",
        "    return VGG(make_layers(cfg['A'], batch_norm=True))\n",
        "\n",
        "\n",
        "def vgg13():\n",
        "    \"\"\"VGG 13-layer model (configuration \"B\")\"\"\"\n",
        "    return VGG(make_layers(cfg['B']))\n",
        "\n",
        "\n",
        "def vgg13_bn():\n",
        "    \"\"\"VGG 13-layer model (configuration \"B\") with batch normalization\"\"\"\n",
        "    return VGG(make_layers(cfg['B'], batch_norm=True))\n",
        "\n",
        "\n",
        "def vgg16():\n",
        "    \"\"\"VGG 16-layer model (configuration \"D\")\"\"\"\n",
        "    return VGG(make_layers(cfg['D']))\n",
        "\n",
        "\n",
        "def vgg16_bn():\n",
        "    \"\"\"VGG 16-layer model (configuration \"D\") with batch normalization\"\"\"\n",
        "    return VGG(make_layers(cfg['D'], batch_norm=True))\n",
        "\n",
        "\n",
        "def vgg19():\n",
        "    \"\"\"VGG 19-layer model (configuration \"E\")\"\"\"\n",
        "    return VGG(make_layers(cfg['E']))\n",
        "\n",
        "\n",
        "def vgg19_bn():\n",
        "    \"\"\"VGG 19-layer model (configuration 'E') with batch normalization\"\"\"\n",
        "    return VGG(make_layers(cfg['E'], batch_norm=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3sqLHJqsUTj",
        "colab_type": "code",
        "outputId": "19c5a51b-d6be-4811-96a1-bd7c0ca56a61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 974
        }
      },
      "source": [
        "# create a complete CNN\n",
        "model = vgg16_bn()\n",
        "print(model)\n",
        "\n",
        "# move tensors to GPU if CUDA is available\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace)\n",
            "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace)\n",
            "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (9): ReLU(inplace)\n",
            "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (12): ReLU(inplace)\n",
            "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (16): ReLU(inplace)\n",
            "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (19): ReLU(inplace)\n",
            "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (22): ReLU(inplace)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (26): ReLU(inplace)\n",
            "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (29): ReLU(inplace)\n",
            "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (32): ReLU(inplace)\n",
            "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (36): ReLU(inplace)\n",
            "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (39): ReLU(inplace)\n",
            "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (42): ReLU(inplace)\n",
            "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Dropout(p=0.5)\n",
            "    (1): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (2): ReLU(inplace)\n",
            "    (3): Dropout(p=0.5)\n",
            "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (5): ReLU(inplace)\n",
            "    (6): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "28qXoiQZS5qj",
        "colab": {}
      },
      "source": [
        "# loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=9, gamma=0.9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kl0M610RTWnl",
        "outputId": "c6d88f5a-b13b-4de0-bd7e-abd6bfa31df7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 874
        }
      },
      "source": [
        "# number of epochs to train the model\n",
        "n_epochs = 35 # you may increase this number to train a final model\n",
        "\n",
        "valid_loss_min = np.Inf # track change in validation loss\n",
        "\n",
        "for epoch in range(1, n_epochs+1):\n",
        "\n",
        "    # keep track of training and validation loss\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "    \n",
        "    ###################\n",
        "    # train the model #\n",
        "    ###################\n",
        "    scheduler.step()\n",
        "    model.train()\n",
        "    for data, target in train_loader:\n",
        "        # move tensors to GPU if CUDA is available\n",
        "        if torch.cuda.is_available():\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        # clear the gradients of all optimized variables\n",
        "        optimizer.zero_grad()\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(data)\n",
        "        # calculate the batch loss\n",
        "        loss = criterion(output, target)\n",
        "        # backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        # perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "        # update training loss\n",
        "        train_loss += loss.item()*data.size(0)\n",
        "        \n",
        "    ######################    \n",
        "    # validate the model #\n",
        "    ######################\n",
        "    model.eval()\n",
        "    for data, target in valid_loader:\n",
        "        # move tensors to GPU if CUDA is available\n",
        "        if torch.cuda.is_available():\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(data)\n",
        "        # calculate the batch loss\n",
        "        loss = criterion(output, target)\n",
        "        # update average validation loss \n",
        "        valid_loss += loss.item()*data.size(0)\n",
        "    \n",
        "    # calculate average losses\n",
        "    train_loss = train_loss/len(train_loader.dataset)\n",
        "    valid_loss = valid_loss/len(valid_loader.dataset)\n",
        "        \n",
        "    # print training/validation statistics \n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
        "        epoch, train_loss, valid_loss))\n",
        "    \n",
        "    # save model if validation loss has decreased\n",
        "    if valid_loss <= valid_loss_min:\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "        valid_loss_min,\n",
        "        valid_loss))\n",
        "        torch.save(model.state_dict(), 'model_cifar.pt')\n",
        "        valid_loss_min = valid_loss"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 \tTraining Loss: 1.521935 \tValidation Loss: 0.141005\n",
            "Validation loss decreased (inf --> 0.141005).  Saving model ...\n",
            "Epoch: 2 \tTraining Loss: 1.067071 \tValidation Loss: 0.101275\n",
            "Validation loss decreased (0.141005 --> 0.101275).  Saving model ...\n",
            "Epoch: 3 \tTraining Loss: 0.855626 \tValidation Loss: 0.098922\n",
            "Validation loss decreased (0.101275 --> 0.098922).  Saving model ...\n",
            "Epoch: 4 \tTraining Loss: 0.738719 \tValidation Loss: 0.081031\n",
            "Validation loss decreased (0.098922 --> 0.081031).  Saving model ...\n",
            "Epoch: 5 \tTraining Loss: 0.655049 \tValidation Loss: 0.073330\n",
            "Validation loss decreased (0.081031 --> 0.073330).  Saving model ...\n",
            "Epoch: 6 \tTraining Loss: 0.591754 \tValidation Loss: 0.061094\n",
            "Validation loss decreased (0.073330 --> 0.061094).  Saving model ...\n",
            "Epoch: 7 \tTraining Loss: 0.545505 \tValidation Loss: 0.065721\n",
            "Epoch: 8 \tTraining Loss: 0.503239 \tValidation Loss: 0.057246\n",
            "Validation loss decreased (0.061094 --> 0.057246).  Saving model ...\n",
            "Epoch: 9 \tTraining Loss: 0.457210 \tValidation Loss: 0.057602\n",
            "Epoch: 11 \tTraining Loss: 0.407142 \tValidation Loss: 0.045113\n",
            "Validation loss decreased (0.049809 --> 0.045113).  Saving model ...\n",
            "Epoch: 12 \tTraining Loss: 0.385679 \tValidation Loss: 0.047643\n",
            "Epoch: 13 \tTraining Loss: 0.370206 \tValidation Loss: 0.043611\n",
            "Validation loss decreased (0.045113 --> 0.043611).  Saving model ...\n",
            "Epoch: 14 \tTraining Loss: 0.343589 \tValidation Loss: 0.044113\n",
            "Epoch: 15 \tTraining Loss: 0.329124 \tValidation Loss: 0.043882\n",
            "Epoch: 16 \tTraining Loss: 0.318037 \tValidation Loss: 0.044875\n",
            "Epoch: 17 \tTraining Loss: 0.299919 \tValidation Loss: 0.042601\n",
            "Validation loss decreased (0.043611 --> 0.042601).  Saving model ...\n",
            "Epoch: 18 \tTraining Loss: 0.276063 \tValidation Loss: 0.042552\n",
            "Validation loss decreased (0.042601 --> 0.042552).  Saving model ...\n",
            "Epoch: 19 \tTraining Loss: 0.258634 \tValidation Loss: 0.044952\n",
            "Epoch: 20 \tTraining Loss: 0.247746 \tValidation Loss: 0.039623\n",
            "Validation loss decreased (0.042552 --> 0.039623).  Saving model ...\n",
            "Epoch: 21 \tTraining Loss: 0.239457 \tValidation Loss: 0.038989\n",
            "Validation loss decreased (0.039623 --> 0.038989).  Saving model ...\n",
            "Epoch: 22 \tTraining Loss: 0.227427 \tValidation Loss: 0.037658\n",
            "Validation loss decreased (0.038989 --> 0.037658).  Saving model ...\n",
            "Epoch: 23 \tTraining Loss: 0.219838 \tValidation Loss: 0.038473\n",
            "Epoch: 24 \tTraining Loss: 0.208932 \tValidation Loss: 0.039884\n",
            "Epoch: 25 \tTraining Loss: 0.199782 \tValidation Loss: 0.039360\n",
            "Epoch: 26 \tTraining Loss: 0.196969 \tValidation Loss: 0.038640\n",
            "Epoch: 27 \tTraining Loss: 0.180557 \tValidation Loss: 0.036029\n",
            "Validation loss decreased (0.037658 --> 0.036029).  Saving model ...\n",
            "Epoch: 28 \tTraining Loss: 0.170582 \tValidation Loss: 0.036702\n",
            "Epoch: 29 \tTraining Loss: 0.161664 \tValidation Loss: 0.035834\n",
            "Validation loss decreased (0.036029 --> 0.035834).  Saving model ...\n",
            "Epoch: 30 \tTraining Loss: 0.157423 \tValidation Loss: 0.036408\n",
            "Epoch: 31 \tTraining Loss: 0.153548 \tValidation Loss: 0.044321\n",
            "Epoch: 32 \tTraining Loss: 0.147481 \tValidation Loss: 0.034050\n",
            "Validation loss decreased (0.035834 --> 0.034050).  Saving model ...\n",
            "Epoch: 33 \tTraining Loss: 0.141469 \tValidation Loss: 0.038963\n",
            "Epoch: 34 \tTraining Loss: 0.136618 \tValidation Loss: 0.038330\n",
            "Epoch: 35 \tTraining Loss: 0.136246 \tValidation Loss: 0.041677\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYBIIn4attEY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loader = torch.utils.data.DataLoader(test_dataset, \n",
        "                                          batch_size=64, \n",
        "                                          shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQIeowPTuQgD",
        "colab_type": "code",
        "outputId": "0ec15a4a-70d7-4838-9f4f-1e50e5bde584",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.load_state_dict(torch.load('model_cifar.pt'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTvAvO53vZOS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classDict = {'plane':0, 'car':1, 'bird':2, 'cat':3, 'deer':4, 'dog':5, 'frog':6, 'horse':7, 'ship':8, 'truck':9}\n",
        "class_selected_ = list(classDict.keys())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iy3CFfVwuU49",
        "colab_type": "code",
        "outputId": "5fae5320-ec55-426a-d24d-0d324412a68f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "test_loss = 0.0 \n",
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "\n",
        "model.eval()\n",
        "for data , target in test_loader:\n",
        "    \n",
        "    if torch.cuda.is_available():\n",
        "        data , target = data.cuda() , target.cuda()\n",
        "        \n",
        "    output = model(data)\n",
        "    \n",
        "    loss = criterion(output , target)\n",
        "    #print(loss)\n",
        "    \n",
        "    test_loss += loss.item()*data.size(0)\n",
        "    #print(test_loss)\n",
        "    \n",
        "    _, pred = torch.max(output, 1) \n",
        "    \n",
        "    correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not torch.cuda.is_available() else np.squeeze(correct_tensor.cpu().numpy())\n",
        "    \n",
        "    for i in range(len(correct)):\n",
        "        label = target.data[i]\n",
        "        class_correct[label] += correct[i].item()\n",
        "        class_total[label] += 1\n",
        "\n",
        "# average test loss\n",
        "test_loss = test_loss/len(test_loader.dataset)\n",
        "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "\n",
        "for i in range(10):\n",
        "    if class_total[i] > 0:\n",
        "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
        "            class_selected_[i], 100 * class_correct[i] / class_total[i],\n",
        "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
        "    else:\n",
        "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
        "\n",
        "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
        "    100. * np.sum(class_correct) / np.sum(class_total),\n",
        "    np.sum(class_correct), np.sum(class_total)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.311279\n",
            "\n",
            "Test Accuracy of plane: 96% (4819/5000)\n",
            "Test Accuracy of   car: 91% (4598/5000)\n",
            "Test Accuracy of  bird: 84% (4238/5000)\n",
            "Test Accuracy of   cat: 81% (4061/5000)\n",
            "Test Accuracy of  deer: 88% (4413/5000)\n",
            "Test Accuracy of   dog: 78% (3902/5000)\n",
            "Test Accuracy of  frog: 91% (4569/5000)\n",
            "Test Accuracy of horse: 93% (4659/5000)\n",
            "Test Accuracy of  ship: 93% (4691/5000)\n",
            "Test Accuracy of truck: 94% (4737/5000)\n",
            "\n",
            "Test Accuracy (Overall): 89% (44687/50000)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}