{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cifar10_Classification_VGG11_.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hamidreza2015/pythorch_experiences/blob/master/Cifar10_Classification_VGG11_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XoZynJvNOqox",
        "colab": {}
      },
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.models as models\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Yqy-nkngQg6j",
        "outputId": "510be84a-a3ea-4c3a-d1fe-9aa132d91137",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Train, Validate, Test. Heavily inspired by Kevinzakka https://github.com/kevinzakka/DenseNet/blob/master/data_loader.py\n",
        "\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "\n",
        "\n",
        "\n",
        "# define transforms\n",
        "valid_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "])\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "])\n",
        "\n",
        "\n",
        "# load the dataset\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, \n",
        "            download=True, transform=train_transform)\n",
        "\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, \n",
        "            download=True, transform=valid_transform)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nK_2yRM0RJAG",
        "colab": {}
      },
      "source": [
        "batch_size = 64\n",
        "\n",
        "valid_size=0.1\n",
        "\n",
        "num_train = len(train_dataset)\n",
        "indices = list(range(num_train))\n",
        "split = int(np.floor(valid_size * num_train))\n",
        "\n",
        "\n",
        "np.random.seed(42)\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
        "                batch_size=batch_size, sampler=train_sampler)\n",
        "\n",
        "valid_loader = torch.utils.data.DataLoader(train_dataset, \n",
        "                batch_size=batch_size, sampler=valid_sampler)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVSo6JJzMtzG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "writer = SummaryWriter()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TyeCGOiMCp8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "images ,labels = next(iter(train_loader))\n",
        "grid = torchvision.utils.make_grid(images)\n",
        "\n",
        "writer.add_image('images' , grid)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Asg_Qm0mrQOy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "\n",
        "__all__ = [\n",
        "    'VGG', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn',\n",
        "    'vgg19_bn', 'vgg19',\n",
        "]\n",
        "\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    '''\n",
        "    VGG model \n",
        "    '''\n",
        "    def __init__(self, features):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = features\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(512, 10),\n",
        "        )\n",
        "         # Initialize weights\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def make_layers(cfg, batch_norm=False):\n",
        "    layers = []\n",
        "    in_channels = 3\n",
        "    for v in cfg:\n",
        "        if v == 'M':\n",
        "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "        else:\n",
        "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
        "            if batch_norm:\n",
        "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
        "            else:\n",
        "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
        "            in_channels = v\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "cfg = {\n",
        "    'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', \n",
        "          512, 512, 512, 512, 'M'],\n",
        "}\n",
        "\n",
        "\n",
        "def vgg11():\n",
        "    \"\"\"VGG 11-layer model (configuration \"A\")\"\"\"\n",
        "    return VGG(make_layers(cfg['A']))\n",
        "\n",
        "\n",
        "def vgg11_bn():\n",
        "    \"\"\"VGG 11-layer model (configuration \"A\") with batch normalization\"\"\"\n",
        "    return VGG(make_layers(cfg['A'], batch_norm=True))\n",
        "\n",
        "\n",
        "def vgg13():\n",
        "    \"\"\"VGG 13-layer model (configuration \"B\")\"\"\"\n",
        "    return VGG(make_layers(cfg['B']))\n",
        "\n",
        "\n",
        "def vgg13_bn():\n",
        "    \"\"\"VGG 13-layer model (configuration \"B\") with batch normalization\"\"\"\n",
        "    return VGG(make_layers(cfg['B'], batch_norm=True))\n",
        "\n",
        "\n",
        "def vgg16():\n",
        "    \"\"\"VGG 16-layer model (configuration \"D\")\"\"\"\n",
        "    return VGG(make_layers(cfg['D']))\n",
        "\n",
        "\n",
        "def vgg16_bn():\n",
        "    \"\"\"VGG 16-layer model (configuration \"D\") with batch normalization\"\"\"\n",
        "    return VGG(make_layers(cfg['D'], batch_norm=True))\n",
        "\n",
        "\n",
        "def vgg19():\n",
        "    \"\"\"VGG 19-layer model (configuration \"E\")\"\"\"\n",
        "    return VGG(make_layers(cfg['E']))\n",
        "\n",
        "\n",
        "def vgg19_bn():\n",
        "    \"\"\"VGG 19-layer model (configuration 'E') with batch normalization\"\"\"\n",
        "    return VGG(make_layers(cfg['E'], batch_norm=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3sqLHJqsUTj",
        "colab_type": "code",
        "outputId": "a6a8748f-e435-460b-83d0-fe1622c7df7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        }
      },
      "source": [
        "# create a complete CNN\n",
        "model = vgg11()\n",
        "print(model)\n",
        "\n",
        "writer.add_graph(model ,input_to_model=images)\n",
        "\n",
        "# move tensors to GPU if CUDA is available\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace)\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): ReLU(inplace)\n",
            "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (7): ReLU(inplace)\n",
            "    (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (9): ReLU(inplace)\n",
            "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (12): ReLU(inplace)\n",
            "    (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (14): ReLU(inplace)\n",
            "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (17): ReLU(inplace)\n",
            "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (19): ReLU(inplace)\n",
            "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Dropout(p=0.5)\n",
            "    (1): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (2): ReLU(inplace)\n",
            "    (3): Dropout(p=0.5)\n",
            "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (5): ReLU(inplace)\n",
            "    (6): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "28qXoiQZS5qj",
        "colab": {}
      },
      "source": [
        "# loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=9, gamma=0.9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lo_SklHaOKB9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_num_correct(preds, labels):\n",
        "    return preds.argmax(dim=1).eq(labels).sum().item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kl0M610RTWnl",
        "outputId": "91261086-6a11-44dc-c925-42638245e427",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 903
        }
      },
      "source": [
        "# number of epochs to train the model\n",
        "n_epochs = 36 # you may increase this number to train a final model\n",
        "\n",
        "valid_loss_min = np.Inf # track change in validation loss\n",
        "\n",
        "for epoch in range(1, n_epochs+1):\n",
        "\n",
        "    # keep track of training and validation loss\n",
        "    train_loss = 0.0\n",
        "    valid_loss = 0.0\n",
        "    total_correct = 0.0\n",
        "    total_loss = 0.0\n",
        "    \n",
        "    ###################\n",
        "    # train the model #\n",
        "    ###################\n",
        "    scheduler.step()\n",
        "    model.train()\n",
        "    for data, target in train_loader:\n",
        "        # move tensors to GPU if CUDA is available\n",
        "        if torch.cuda.is_available():\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        # clear the gradients of all optimized variables\n",
        "        optimizer.zero_grad()\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(data)\n",
        "        # calculate the batch loss\n",
        "        loss = criterion(output, target)\n",
        "        # backward pass: compute gradient of the loss with respect to model parameters\n",
        "        loss.backward()\n",
        "        # perform a single optimization step (parameter update)\n",
        "        optimizer.step()\n",
        "        # update training loss\n",
        "        total_loss += loss.item()*data.size(0)\n",
        "        \n",
        "    ######################    \n",
        "    # validate the model #\n",
        "    ######################\n",
        "    model.eval()\n",
        "    for data, target in valid_loader:\n",
        "        # move tensors to GPU if CUDA is available\n",
        "        if torch.cuda.is_available():\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(data)\n",
        "        # calculate the batch loss\n",
        "        loss = criterion(output, target)\n",
        "        # update average validation loss \n",
        "        valid_loss += loss.item()*data.size(0)\n",
        "        \n",
        "        total_correct += get_num_correct(output, target)\n",
        "    \n",
        "    # calculate average losses\n",
        "    train_loss = total_loss/len(train_loader.dataset)\n",
        "    valid_loss = valid_loss/len(valid_loader.dataset)\n",
        "    \n",
        "       \n",
        "    \n",
        "    writer.add_scalar('Loss',total_loss, epoch)\n",
        "    writer.add_scalar('Number Correct',total_correct, epoch)\n",
        "    writer.add_scalar('Accuracy', (total_correct/(len(train_dataset)*0.1)) , epoch)\n",
        "    \n",
        "    \n",
        "    \n",
        "    # print training/validation statistics \n",
        "    print('Epoch: {} Total_Loss: {:.6f} Training_Loss: {:.6f} Validation_Loss: {:.6f} Total_Correct: {}'.format(\n",
        "        epoch, total_loss ,train_loss, valid_loss ,total_correct))\n",
        "    \n",
        "    # save model if validation loss has decreased\n",
        "    if valid_loss <= valid_loss_min:\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
        "        valid_loss_min,\n",
        "        valid_loss))\n",
        "        torch.save(model.state_dict(), 'model_cifar.pt')\n",
        "        valid_loss_min = valid_loss"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1 Total_Loss: 90919.046750 Training_Loss: 1.818381 Validation_Loss: 0.180438 Total_Correct: 1462.0\n",
            "Validation loss decreased (inf --> 0.180438).  Saving model ...\n",
            "Epoch: 2 Total_Loss: 71364.685709 Training_Loss: 1.427294 Validation_Loss: 0.142726 Total_Correct: 2355.0\n",
            "Validation loss decreased (0.180438 --> 0.142726).  Saving model ...\n",
            "Epoch: 3 Total_Loss: 59453.760228 Training_Loss: 1.189075 Validation_Loss: 0.125552 Total_Correct: 2660.0\n",
            "Validation loss decreased (0.142726 --> 0.125552).  Saving model ...\n",
            "Epoch: 4 Total_Loss: 50140.912100 Training_Loss: 1.002818 Validation_Loss: 0.101283 Total_Correct: 3255.0\n",
            "Validation loss decreased (0.125552 --> 0.101283).  Saving model ...\n",
            "Epoch: 5 Total_Loss: 43990.569581 Training_Loss: 0.879811 Validation_Loss: 0.093109 Total_Correct: 3398.0\n",
            "Validation loss decreased (0.101283 --> 0.093109).  Saving model ...\n",
            "Epoch: 6 Total_Loss: 39761.426718 Training_Loss: 0.795229 Validation_Loss: 0.081860 Total_Correct: 3560.0\n",
            "Validation loss decreased (0.093109 --> 0.081860).  Saving model ...\n",
            "Epoch: 7 Total_Loss: 35957.531703 Training_Loss: 0.719151 Validation_Loss: 0.073537 Total_Correct: 3738.0\n",
            "Validation loss decreased (0.081860 --> 0.073537).  Saving model ...\n",
            "Epoch: 8 Total_Loss: 33644.660913 Training_Loss: 0.672893 Validation_Loss: 0.074952 Total_Correct: 3687.0\n",
            "Epoch: 9 Total_Loss: 30717.619009 Training_Loss: 0.614352 Validation_Loss: 0.068217 Total_Correct: 3839.0\n",
            "Validation loss decreased (0.073537 --> 0.068217).  Saving model ...\n",
            "Epoch: 10 Total_Loss: 28391.862533 Training_Loss: 0.567837 Validation_Loss: 0.068917 Total_Correct: 3839.0\n",
            "Epoch: 11 Total_Loss: 26389.630840 Training_Loss: 0.527793 Validation_Loss: 0.067575 Total_Correct: 3880.0\n",
            "Validation loss decreased (0.068217 --> 0.067575).  Saving model ...\n",
            "Epoch: 12 Total_Loss: 25022.187185 Training_Loss: 0.500444 Validation_Loss: 0.061398 Total_Correct: 3993.0\n",
            "Validation loss decreased (0.067575 --> 0.061398).  Saving model ...\n",
            "Epoch: 13 Total_Loss: 23875.167515 Training_Loss: 0.477503 Validation_Loss: 0.058362 Total_Correct: 4005.0\n",
            "Validation loss decreased (0.061398 --> 0.058362).  Saving model ...\n",
            "Epoch: 14 Total_Loss: 22627.345160 Training_Loss: 0.452547 Validation_Loss: 0.059344 Total_Correct: 4002.0\n",
            "Epoch: 15 Total_Loss: 21188.550709 Training_Loss: 0.423771 Validation_Loss: 0.059345 Total_Correct: 4018.0\n",
            "Epoch: 16 Total_Loss: 20212.011099 Training_Loss: 0.404240 Validation_Loss: 0.053900 Total_Correct: 4096.0\n",
            "Validation loss decreased (0.058362 --> 0.053900).  Saving model ...\n",
            "Epoch: 17 Total_Loss: 19470.020092 Training_Loss: 0.389400 Validation_Loss: 0.055121 Total_Correct: 4070.0\n",
            "Epoch: 18 Total_Loss: 17807.055838 Training_Loss: 0.356141 Validation_Loss: 0.052253 Total_Correct: 4145.0\n",
            "Validation loss decreased (0.053900 --> 0.052253).  Saving model ...\n",
            "Epoch: 19 Total_Loss: 17068.767880 Training_Loss: 0.341375 Validation_Loss: 0.048961 Total_Correct: 4152.0\n",
            "Validation loss decreased (0.052253 --> 0.048961).  Saving model ...\n",
            "Epoch: 20 Total_Loss: 16191.386897 Training_Loss: 0.323828 Validation_Loss: 0.054536 Total_Correct: 4115.0\n",
            "Epoch: 21 Total_Loss: 15671.385944 Training_Loss: 0.313428 Validation_Loss: 0.049437 Total_Correct: 4173.0\n",
            "Epoch: 22 Total_Loss: 15043.010542 Training_Loss: 0.300860 Validation_Loss: 0.048876 Total_Correct: 4213.0\n",
            "Validation loss decreased (0.048961 --> 0.048876).  Saving model ...\n",
            "Epoch: 23 Total_Loss: 14613.353343 Training_Loss: 0.292267 Validation_Loss: 0.050883 Total_Correct: 4179.0\n",
            "Epoch: 24 Total_Loss: 13802.466707 Training_Loss: 0.276049 Validation_Loss: 0.047352 Total_Correct: 4227.0\n",
            "Validation loss decreased (0.048876 --> 0.047352).  Saving model ...\n",
            "Epoch: 25 Total_Loss: 13202.354960 Training_Loss: 0.264047 Validation_Loss: 0.051430 Total_Correct: 4167.0\n",
            "Epoch: 26 Total_Loss: 13362.612237 Training_Loss: 0.267252 Validation_Loss: 0.052053 Total_Correct: 4183.0\n",
            "Epoch: 27 Total_Loss: 11800.514861 Training_Loss: 0.236010 Validation_Loss: 0.045513 Total_Correct: 4285.0\n",
            "Validation loss decreased (0.047352 --> 0.045513).  Saving model ...\n",
            "Epoch: 28 Total_Loss: 11342.900729 Training_Loss: 0.226858 Validation_Loss: 0.052744 Total_Correct: 4199.0\n",
            "Epoch: 29 Total_Loss: 10973.429055 Training_Loss: 0.219469 Validation_Loss: 0.049610 Total_Correct: 4232.0\n",
            "Epoch: 30 Total_Loss: 10669.433402 Training_Loss: 0.213389 Validation_Loss: 0.050246 Total_Correct: 4264.0\n",
            "Epoch: 31 Total_Loss: 10247.732130 Training_Loss: 0.204955 Validation_Loss: 0.047416 Total_Correct: 4256.0\n",
            "Epoch: 32 Total_Loss: 9367.808053 Training_Loss: 0.187356 Validation_Loss: 0.049663 Total_Correct: 4253.0\n",
            "Epoch: 33 Total_Loss: 9395.077006 Training_Loss: 0.187902 Validation_Loss: 0.053073 Total_Correct: 4209.0\n",
            "Epoch: 34 Total_Loss: 9099.703784 Training_Loss: 0.181994 Validation_Loss: 0.050373 Total_Correct: 4259.0\n",
            "Epoch: 35 Total_Loss: 8591.802734 Training_Loss: 0.171836 Validation_Loss: 0.048601 Total_Correct: 4285.0\n",
            "Epoch: 36 Total_Loss: 8007.002228 Training_Loss: 0.160140 Validation_Loss: 0.051081 Total_Correct: 4265.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "900YgEVvQBgF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "be8ddd1d-c25b-4549-b08d-1e6e8f1eaef8"
      },
      "source": [
        "(total_correct/(len(train_dataset)*0.1))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6428"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYBIIn4attEY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loader = torch.utils.data.DataLoader(test_dataset, \n",
        "                                          batch_size=64, \n",
        "                                          shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQIeowPTuQgD",
        "colab_type": "code",
        "outputId": "676c1610-69f6-4bb8-b84d-b7cc352b042c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "model.load_state_dict(torch.load('model_cifar.pt'))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTvAvO53vZOS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classDict = {'plane':0, 'car':1, 'bird':2, 'cat':3, 'deer':4, 'dog':5, 'frog':6, 'horse':7, 'ship':8, 'truck':9}\n",
        "class_selected_ = list(classDict.keys())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iy3CFfVwuU49",
        "colab_type": "code",
        "outputId": "431d1ee1-bf5d-4679-c62c-eb283824404b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "source": [
        "test_loss = 0.0 \n",
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "\n",
        "model.eval()\n",
        "for data , target in test_loader:\n",
        "    \n",
        "    if torch.cuda.is_available():\n",
        "        data , target = data.cuda() , target.cuda()\n",
        "        \n",
        "    output = model(data)\n",
        "    \n",
        "    loss = criterion(output , target)\n",
        "    #print(loss)\n",
        "    \n",
        "    test_loss += loss.item()*data.size(0)\n",
        "    #print(test_loss)\n",
        "    \n",
        "    _, pred = torch.max(output, 1) \n",
        "    \n",
        "    correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not torch.cuda.is_available() else np.squeeze(correct_tensor.cpu().numpy())\n",
        "    \n",
        "    for i in range(len(correct)):\n",
        "        label = target.data[i]\n",
        "        class_correct[label] += correct[i].item()\n",
        "        class_total[label] += 1\n",
        "\n",
        "# average test loss\n",
        "test_loss = test_loss/len(test_loader.dataset)\n",
        "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "\n",
        "for i in range(10):\n",
        "    if class_total[i] > 0:\n",
        "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
        "            class_selected_[i], 100 * class_correct[i] / class_total[i],\n",
        "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
        "    else:\n",
        "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
        "\n",
        "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
        "    100. * np.sum(class_correct) / np.sum(class_total),\n",
        "    np.sum(class_correct), np.sum(class_total)))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.448277\n",
            "\n",
            "Test Accuracy of plane: 90% (907/1000)\n",
            "Test Accuracy of   car: 97% (970/1000)\n",
            "Test Accuracy of  bird: 80% (807/1000)\n",
            "Test Accuracy of   cat: 70% (706/1000)\n",
            "Test Accuracy of  deer: 84% (845/1000)\n",
            "Test Accuracy of   dog: 78% (785/1000)\n",
            "Test Accuracy of  frog: 85% (855/1000)\n",
            "Test Accuracy of horse: 92% (925/1000)\n",
            "Test Accuracy of  ship: 91% (913/1000)\n",
            "Test Accuracy of truck: 88% (886/1000)\n",
            "\n",
            "Test Accuracy (Overall): 85% (8599/10000)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}